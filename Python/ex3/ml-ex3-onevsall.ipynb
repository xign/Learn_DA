{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One vs All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.optimize\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and visualizing training data\n",
    "\n",
    "The training data is 5000 digit images of digits of size 20x20. We will display a random selection of 25 of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ex3data1 = scipy.io.loadmat(\"./ex3data1.mat\")\n",
    "X = ex3data1['X']\n",
    "y = ex3data1['y'][:,0]\n",
    "y[y==10] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m, n = X.shape\n",
    "m, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "fig.subplots_adjust(wspace=0.05, hspace=0.15)\n",
    "\n",
    "import random\n",
    "\n",
    "display_rows, display_cols = (5, 5)\n",
    "\n",
    "for i in range(display_rows * display_cols):\n",
    "    ax = fig.add_subplot(display_rows, display_cols, i+1)\n",
    "    ax.set_axis_off()\n",
    "    image = X[random.randint(0, m-1)].reshape(20, 20).T\n",
    "    image /= np.max(image)\n",
    "    ax.imshow(image, cmap=plt.cm.Greys_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.insert(X, 0, np.ones(m), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 2: Vectorize Logistic Regression\n",
    "In this part of the exercise, you will reuse your logistic regression\n",
    "code from the last exercise. You task here is to make sure that your\n",
    "regularized logistic regression implementation is vectorized. After\n",
    "that, you will implement one-vs-all classification for the handwritten\n",
    "digit dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def h(theta, x):\n",
    "    return sigmoid(x.dot(theta))\n",
    "\n",
    "#LRCOSTFUNCTION Compute cost and gradient for logistic regression with \n",
    "#regularization\n",
    "#   J = LRCOSTFUNCTION(theta, X, y, lambda) computes the cost of using\n",
    "#   theta as the parameter for regularized logistic regression and the\n",
    "#   gradient of the cost w.r.t. to the parameters. \n",
    "\n",
    "def cost(X, y, theta, lambda_=None):\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Compute the cost of a particular choice of theta.\n",
    "    #               You should set J to the cost.\n",
    "    #               Compute the partial derivatives and set grad to the partial\n",
    "    #               derivatives of the cost w.r.t. each parameter in theta\n",
    "    #\n",
    "    # Hint: The computation of the cost function and gradients can be\n",
    "    #       efficiently vectorized. For example, consider the computation\n",
    "    #\n",
    "    #           sigmoid(X * theta)\n",
    "    #\n",
    "    #       Each row of the resulting matrix will contain the value of the\n",
    "    #       prediction for that example. You can make use of this to vectorize\n",
    "    #       the cost function and gradient computations. \n",
    "    #\n",
    "\n",
    "    \n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    return J\n",
    "\n",
    "def gradient(X, y, theta, lambda_=None):\n",
    "    # You need to return the following variables correctly     \n",
    "    grad = np.zeros(theta.shape)\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Hint: When computing the gradient of the regularized cost function, \n",
    "    #       there're many possible vectorized solutions, but one solution\n",
    "    #       looks like:\n",
    "    #           grad = (unregularized gradient for logistic regression)\n",
    "    #           temp = theta; \n",
    "    #           temp[0] = 0;   # because we don't add anything for j = 0  \n",
    "    #           grad = grad + YOUR_CODE_HERE (using the temp variable)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_theta = np.zeros(n + 1)\n",
    "lambda_ = 0.1\n",
    "cost(X, y, initial_theta, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradient(X, y, initial_theta, lambda_).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_vs_all(X, y, num_labels, lambda_):\n",
    "    #ONEVSALL trains multiple logistic regression classifiers and returns all\n",
    "    #the classifiers in a matrix all_theta, where the i-th row of all_theta \n",
    "    #corresponds to the classifier for label i\n",
    "    #   [all_theta] = ONEVSALL(X, y, num_labels, lambda) trains num_labels\n",
    "    #   logisitc regression classifiers and returns each of these classifiers\n",
    "    #   in a list all_theta, where the i-th item of all_theta corresponds \n",
    "    #   to the classifier for label i\n",
    "    \n",
    "    # You need to return the following variables correctly \n",
    "    all_theta = [None] * num_labels\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: You should complete the following code to train num_labels\n",
    "    #               logistic regression classifiers with regularization\n",
    "    #               parameter lambda. \n",
    "    #\n",
    "    # Hint: You can use y == c to obtain a vector of True's and False's\n",
    "    #\n",
    "    # Note: For this assignment, we recommend using scipy.optimize.minimize with method='L-BFGS-B'\n",
    "    #       to optimize the cost function.\n",
    "    #       It is okay to use a for-loop (for i in range(num_labels)) to\n",
    "    #       loop over the different classes.\n",
    "    #\n",
    "    # Example Code for scipy.optimize.minimize:\n",
    "    #\n",
    "    #     result = scipy.optimize.minimize(lambda t: cost(X, y==digit, t, lambda_),\n",
    "    #                                  initial_theta,\n",
    "    #                                  jac=lambda t: gradient(X, y==digit, t, lambda_),\n",
    "    #                                  method='L-BFGS-B')\n",
    "    #     theta = result.x\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_labels = 10\n",
    "thetas = one_vs_all(X, y, num_labels, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "for d in range(10):\n",
    "    ax = fig.add_subplot(5, 2, d+1)\n",
    "    ax.scatter(range(m), h(thetas[d], X), s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_one_vs_all(X, thetas):\n",
    "    #PREDICT Predict the label for a trained one-vs-all classifier. The labels \n",
    "    #are in the range 1..K, where K = len(thetas)\n",
    "    #  p = PREDICTONEVSALL(all_theta, X) will return a vector of predictions\n",
    "    #  for each example in the matrix X. Note that X contains the examples in\n",
    "    #  rows. all_theta is a list where the i-th entry is a trained logistic\n",
    "    #  regression theta vector for the i-th class. You should set p to a vector\n",
    "    #  of values from 1..K (e.g., p = [1; 3; 1; 2] predicts classes 1, 3, 1, 2\n",
    "    #  for 4 examples) \n",
    "    \n",
    "    \n",
    "    # You need to return the following variables correctly \n",
    "    p = np.zeros(X.shape[0]);\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Complete the following code to make predictions using\n",
    "    #               your learned logistic regression parameters (one-vs-all).\n",
    "    #               You should set p to a vector of predictions (from 1 to\n",
    "    #               num_labels).\n",
    "    #\n",
    "    # Hint: This code can be done all vectorized using the max function.\n",
    "    #       In particular, the max function can also return the index of the \n",
    "    #       max element, for more information see 'help max'. If your examples \n",
    "    #       are in rows, then, you can use max(A, [], 2) to obtain the max \n",
    "    #       for each row.\n",
    "    #       \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # =========================================================================\n",
    "                       \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = predict_one_vs_all(X, thetas)\n",
    "\n",
    "plt.scatter(range(m), predictions, s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(predictions == y).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

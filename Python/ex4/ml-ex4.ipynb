{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Neural Network Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment for console - useful for debugging\n",
    "# %qtconsole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ex3data1 = scipy.io.loadmat(\"./ex4data1.mat\")\n",
    "X = ex3data1['X']\n",
    "y = ex3data1['y'][:,0]\n",
    "m, n = X.shape\n",
    "m, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_layer_size  = n  # 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25 # 25 hidden units\n",
    "num_labels = 10        # 10 labels, from 1 to 10\n",
    "                       # (note that we have mapped \"0\" to label 10)\n",
    "lambda_ = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading and Visualizing Data\n",
    "We start the exercise by first loading and visualizing the dataset. You will be working with a dataset that contains handwritten digits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display(X, display_rows=5, display_cols=5, figsize=(4,4), random_x=False):\n",
    "    m = X.shape[0]\n",
    "    fig, axes = plt.subplots(display_rows, display_cols, figsize=figsize)\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "\n",
    "    import random\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.set_axis_off()\n",
    "        x = None\n",
    "        if random_x:\n",
    "            x = random.randint(0, m-1)\n",
    "        else:\n",
    "            x = i\n",
    "        image = X[x].reshape(20, 20).T\n",
    "        image = image / np.max(image)\n",
    "        ax.imshow(image, cmap=plt.cm.Greys_r)\n",
    "\n",
    "display(X, random_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_ones_column(array):\n",
    "    return np.insert(array, 0, 1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Loading Parameters \n",
    "In this part of the exercise, we load some pre-initialized \n",
    "neural network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ex4weights = scipy.io.loadmat('./ex4weights.mat')\n",
    "Theta1 = ex4weights['Theta1']\n",
    "Theta2 = ex4weights['Theta2']\n",
    "print(Theta1.shape, Theta2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unrolling the parameters into one vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn_params = np.concatenate((Theta1.flat, Theta2.flat))\n",
    "nn_params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  Part 3: Compute Cost (Feedforward) \n",
    "  To the neural network, you should first start by implementing the\n",
    "  feedforward part of the neural network that returns the cost only. You\n",
    "  should complete the code in nn_cost_function() to return cost. After\n",
    "  implementing the feedforward to compute the cost, you can verify that\n",
    "  your implementation is correct by verifying that you get the same cost\n",
    "  as us for the fixed debugging parameters.\n",
    "\n",
    "  We suggest implementing the feedforward cost *without* regularization\n",
    "  first so that it will be easier for you to debug. Later, in part 4, you\n",
    "  will get to implement the regularized cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_cost_function(nn_params, input_layer_size, hidden_layer_size,\n",
    "                     num_labels, X, y, lambda_):\n",
    "    #NNCOSTFUNCTION Implements the neural network cost function for a two layer\n",
    "    #neural network which performs classification\n",
    "    #   [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...\n",
    "    #   X, y, lambda) computes the cost and gradient of the neural network. The\n",
    "    #   parameters for the neural network are \"unrolled\" into the vector\n",
    "    #   nn_params and need to be converted back into the weight matrices. \n",
    "    # \n",
    "    #   The returned parameter grad should be a \"unrolled\" vector of the\n",
    "    #   partial derivatives of the neural network.\n",
    "    #\n",
    "\n",
    "    # Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices\n",
    "    # for our 2 layer neural network\n",
    "    t1_len = (input_layer_size+1)*hidden_layer_size\n",
    "    Theta1 = nn_params[:t1_len].reshape(hidden_layer_size, input_layer_size+1)\n",
    "    Theta2 = nn_params[t1_len:].reshape(num_labels, hidden_layer_size+1)\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # You need to return the following variables correctly \n",
    "    J = 0;\n",
    "    Theta1_grad = np.zeros(Theta1.shape);\n",
    "    Theta2_grad = np.zeros(Theta2.shape);\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: You should complete the code by working through the\n",
    "    #               following parts.\n",
    "    #\n",
    "    # Part 1: Feedforward the neural network and return the cost in the\n",
    "    #         variable J. After implementing Part 1, you can verify that your\n",
    "    #         cost function computation is correct by verifying the cost\n",
    "    #         computed for lambda == 0.\n",
    "    #\n",
    "    # Part 2: Implement the backpropagation algorithm to compute the gradients\n",
    "    #         Theta1_grad and Theta2_grad. You should return the partial derivatives of\n",
    "    #         the cost function with respect to Theta1 and Theta2 in Theta1_grad and\n",
    "    #         Theta2_grad, respectively. After implementing Part 2, you can check\n",
    "    #         that your implementation is correct by running checkNNGradients\n",
    "    #\n",
    "    #         Note: The vector y passed into the function is a vector of labels\n",
    "    #               containing values from 1..K. You need to map this vector into a \n",
    "    #               binary vector of 1's and 0's to be used with the neural network\n",
    "    #               cost function.\n",
    "    #\n",
    "    #         Hint: We recommend implementing backpropagation using a for-loop\n",
    "    #               over the training examples if you are implementing it for the \n",
    "    #               first time.\n",
    "    #\n",
    "    # Part 3: Implement regularization with the cost function and gradients.\n",
    "    #\n",
    "    #         Hint: You can implement this around the code for\n",
    "    #               backpropagation. That is, you can compute the gradients for\n",
    "    #               the regularization separately and then add them to Theta1_grad\n",
    "    #               and Theta2_grad from Part 2.\n",
    "    #\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # =========================================================================\n",
    "\n",
    "    # Unroll gradients\n",
    "    gradient = np.concatenate((Theta1_grad.flat, Theta2_grad.flat))\n",
    "\n",
    "    return J, gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost at the given parameters should be about `0.287629`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambda_ = 0       # No regularization\n",
    "nn_cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost at the given parameters and a regularization factor of 1 should be about `0.38377`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 4: Implement Regularization \n",
    "  Once your cost function implementation is correct, you should now\n",
    "  continue to implement the regularization with the cost.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambda_ = 1\n",
    "nn_cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Sigmoid Gradient  \n",
    " Before you start implementing the neural network, you will first\n",
    "  implement the gradient for the sigmoid function. You should complete the\n",
    "  code in sigmoid_gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    #SIGMOIDGRADIENT returns the gradient of the sigmoid function\n",
    "    #evaluated at z\n",
    "    #   g = SIGMOIDGRADIENT(z) computes the gradient of the sigmoid function\n",
    "    #   evaluated at z. This should work regardless if z is a matrix or a\n",
    "    #   vector. In particular, if z is a vector or matrix, you should return\n",
    "    #   the gradient for each element.\n",
    "\n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Compute the gradient of the sigmoid function evaluated at\n",
    "    #               each value of z (z can be a matrix, vector or scalar).\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # =============================================================\n",
    "\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sigmoid_gradient(np.array([1, -0.5, 0, 0.5, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Initializing Pameters \n",
    "  In this part of the exercise, you will be starting to implment a two\n",
    "  layer neural network that classifies digits. You will start by\n",
    "  implementing a function to initialize the weights of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rand_initialize_weight(L_in, L_out):\n",
    "    #RANDINITIALIZEWEIGHTS Randomly initialize the weights of a layer with L_in\n",
    "    #incoming connections and L_out outgoing connections\n",
    "    #   W = RANDINITIALIZEWEIGHTS(L_in, L_out) randomly initializes the weights \n",
    "    #   of a layer with L_in incoming connections and L_out outgoing \n",
    "    #   connections. \n",
    "    #\n",
    "    #   Note that W should be set to a matrix of size(L_out, 1 + L_in) as\n",
    "    #   the column row of W handles the \"bias\" terms\n",
    "    #\n",
    "    \n",
    "    # You need to return the following variables correctly \n",
    "    W = np.zeros((L_out, L_in))\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Initialize W randomly so that we break the symmetry while\n",
    "    #               training the neural network.\n",
    "    #\n",
    "    # Note: The first row of W corresponds to the parameters for the bias units\n",
    "    #\n",
    "    \n",
    "    return W\n",
    "    \n",
    "    # ========================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Implement Backpropagation \n",
    "  Once your cost matches up with ours, you should proceed to implement the\n",
    "  backpropagation algorithm for the neural network. You should add to the\n",
    "  code you've written in nn_cost_function to return the partial\n",
    "  derivatives of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x, dx=1e-6):\n",
    "    perturb = np.zeros(x.size)\n",
    "    result  = np.zeros(x.size)\n",
    "    for i in range(x.size):\n",
    "        perturb[i] = dx\n",
    "        result[i] = (f(x+perturb) - f(x-perturb)) / (2*dx)\n",
    "        perturb[i] = 0\n",
    "    return result\n",
    "\n",
    "def check_NN_gradients(lambda_=0):\n",
    "    input_layer_size = 3\n",
    "    hidden_layer_size = 5\n",
    "    num_labels = 3\n",
    "    m = 5\n",
    "\n",
    "    def debug_matrix(fan_out, fan_in):\n",
    "        W = np.sin(np.arange(fan_out * (fan_in+1))+1) / 10\n",
    "        return W.reshape(fan_out, fan_in+1)\n",
    "\n",
    "    Theta1 = debug_matrix(hidden_layer_size, input_layer_size)\n",
    "    Theta2 = debug_matrix(num_labels, hidden_layer_size)\n",
    "\n",
    "    X = debug_matrix(m, input_layer_size - 1)\n",
    "    y = 1 + ((1 + np.arange(m)) % num_labels)\n",
    "    \n",
    "    nn_params = np.concatenate([Theta1.flat, Theta2.flat])\n",
    "\n",
    "    cost, grad = nn_cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_)\n",
    "    def just_cost(nn_params):\n",
    "        cost, grad = nn_cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_)\n",
    "        return cost\n",
    "    \n",
    "    return np.sum(np.abs(grad - numerical_gradient(just_cost, nn_params))) / grad.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your backpropagation implementation is correct, then the relative difference will be small (less than 1e-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_NN_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_Theta1 = rand_initialize_weight(hidden_layer_size, input_layer_size+1)\n",
    "initial_Theta2 = rand_initialize_weight(num_labels, hidden_layer_size+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Implement Regularization \n",
    "  Once your backpropagation implementation is correct, you should now\n",
    "  continue to implement the regularization with the cost and gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cost_fun(nn_params):\n",
    "    return nn_cost_function(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_)\n",
    "\n",
    "lambda_ = 3\n",
    "nn_params = np.concatenate((initial_Theta1.flat, initial_Theta2.flat))\n",
    "res = scipy.optimize.minimize(cost_fun, nn_params, jac=True, method='L-BFGS-B', \n",
    "                              options=dict(maxiter=200, disp=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost at lambda = 3 should be about 0.57."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res.fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Training NN \n",
    "  You have now implemented all the code necessary to train a neural \n",
    "  network. To train your neural network, we will use scipy.optimize.minimize. \n",
    "  \n",
    "  Recall that these\n",
    "  advanced optimizers are able to train our cost functions efficiently as\n",
    "  long as we provide them with the gradient computations.\n",
    "\n",
    "  After you have completed the assignment, change the MaxIter to a larger\n",
    "  value to see how more training helps. You should also try different values of lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambda_ = 1\n",
    "nn_params = np.concatenate((initial_Theta1.flat, initial_Theta2.flat))\n",
    "res = scipy.optimize.minimize(cost_fun, nn_params, jac=True, method='L-BFGS-B', \n",
    "                              options=dict(maxiter=200, disp=True))\n",
    "nn_params = res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain Theta1 and Theta2 back from nn_params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1_len = (input_layer_size+1)*hidden_layer_size\n",
    "Theta1 = nn_params[:t1_len].reshape(hidden_layer_size, input_layer_size+1)\n",
    "Theta2 = nn_params[t1_len:].reshape(num_labels, hidden_layer_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def raise_me(*args, **kw):\n",
    "    raise Exception('ping')\n",
    "\n",
    "np.ma.MaskedArray.squeeze = raise_me\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Visualize Weights \n",
    "  You can now \"visualize\" what the neural network is learning by \n",
    "  displaying the hidden units to see what features they are capturing in \n",
    "  the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(Theta1[:,1:], figsize=(6,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(Theta1, Theta2, X):\n",
    "    #PREDICT Predict the label of an input given a trained neural network\n",
    "    #   p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the\n",
    "    #   trained weights of a neural network (Theta1, Theta2)\n",
    "\n",
    "    m = X.shape[0]\n",
    "    num_labels = Theta2.shape[1]\n",
    "    \n",
    "    # You need to return the following variables correctly. Remember that \n",
    "    # the given data labels go from 1..10, with 10 representing the digit 0!\n",
    "    p = np.zeros(X.shape[0])\n",
    "\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ============================================================\n",
    "    \n",
    "    \n",
    "    return p\n",
    "\n",
    "predictions = predict(Theta1, Theta2, X)\n",
    "np.mean(predictions == y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

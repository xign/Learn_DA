{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exercise 5 | Regularized Linear Regression and Bias-Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 1: Loading and Visualizing Data \n",
    "  We start the exercise by first loading and visualizing the dataset. \n",
    "  The following code will load the dataset into your environment and plot\n",
    "  the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ex5data1 = scipy.io.loadmat('ex5data1.mat')\n",
    "X = ex5data1['X']\n",
    "y = ex5data1['y'][:,0]\n",
    "Xtest = ex5data1['Xtest']\n",
    "ytest = ex5data1['ytest'][:,0]\n",
    "Xval = ex5data1['Xval']\n",
    "yval = ex5data1['yval'][:,0]\n",
    "print(X.shape, Xtest.shape, Xval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(X, y, marker='x', c='r')\n",
    "ax.set_xlabel('Change in water level')\n",
    "ax.set_ylabel('Water flowing out of the dam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Regularized Linear Regression Cost \n",
    "  You should now implement the cost function for regularized linear \n",
    "  regression. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linearRegCostFunction(X, y, theta, lambda_):\n",
    "    #LINEARREGCOSTFUNCTION Compute cost and gradient for regularized linear \n",
    "    #regression with multiple variables\n",
    "    #   [J, grad] = LINEARREGCOSTFUNCTION(X, y, theta, lambda) computes the \n",
    "    #   cost of using theta as the parameter for linear regression to fit the \n",
    "    #   data points in X and y. Returns the cost in J and the gradient in grad\n",
    "    \n",
    "    m = len(y) # number of training examples\n",
    "    \n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "    \n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Compute the cost and gradient of regularized linear \n",
    "    #               regression for a particular choice of theta.\n",
    "    #\n",
    "    #               You should set J to the cost and grad to the gradient.\n",
    "    #\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ============================================================\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = np.array([1, 1])\n",
    "J, grad = linearRegCostFunction(np.hstack([np.ones((m, 1)), X]), y, theta, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost at theta = `[1, 1]` should be about 303.993192."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient at theta = `[1, 1]` should be about `[-15.303016; 598.250744]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Regularized Linear Regression Gradient \n",
    "  You should now implement the gradient for regularized linear \n",
    " regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train Linear Regression \n",
    "  Once you have implemented the cost and gradient correctly, the\n",
    "  trainLinearRegression function will use your cost function to train \n",
    "  regularized linear regression.\n",
    " \n",
    "  Write Up Note: The data is non-linear, so this will not give a great \n",
    "                 fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainLinearRegression(X, y, lambda_):\n",
    "    initial_theta = np.zeros(X.shape[1])\n",
    "    \n",
    "    def costFunction(t):\n",
    "        return linearRegCostFunction(X, y, t, lambda_)\n",
    "    \n",
    "    res = scipy.optimize.minimize(costFunction, initial_theta, jac=True, method='L-BFGS-B',\n",
    "                                  options=dict(maxiter=200))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = trainLinearRegression(np.hstack([np.ones((m, 1)), X]), y, 0)\n",
    "print(res)\n",
    "optimal_theta = res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot fit over the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(X, y, marker='x', c='r')\n",
    "ax.set_xlabel('Change in water level')\n",
    "ax.set_ylabel('Water flowing out of the dam')\n",
    "ax.plot(X, np.hstack([np.ones((m, 1)), X]).dot(optimal_theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## =========== Part 5: Learning Curve for Linear Regression =============\n",
    "  Next, you should implement the learning_curve function. \n",
    "\n",
    "  Write Up Note: Since the model is underfitting the data, we expect to\n",
    "                 see a graph with \"high bias\" -- slide 8 in ML-advice.pdf \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learning_curve(X, y, Xval, yval, lambda_):\n",
    "    #LEARNINGCURVE Generates the train and cross validation set errors needed \n",
    "    #to plot a learning curve\n",
    "    #   [error_train, error_val] = ...\n",
    "    #       LEARNINGCURVE(X, y, Xval, yval, lambda) returns the train and\n",
    "    #       cross validation set errors for a learning curve. In particular, \n",
    "    #       it returns two vectors of the same length - error_train and \n",
    "    #       error_val. Then, error_train(i) contains the training error for\n",
    "    #       i examples (and similarly for error_val(i)).\n",
    "    #\n",
    "    #   In this function, you will compute the train and test errors for\n",
    "    #   dataset sizes from 1 up to m. In practice, when working with larger\n",
    "    #   datasets, you might want to do this in larger intervals.\n",
    "    #\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # You need to return these values correctly\n",
    "    error_train = np.zeros(m-1)\n",
    "    error_val   = np.zeros(m-1)\n",
    "    \n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Fill in this function to return training errors in \n",
    "    #               error_train and the cross validation errors in error_val. \n",
    "    #               i.e., error_train[i] and \n",
    "    #               error_val[i] should give you the errors\n",
    "    #               obtained after training on i+1 examples.\n",
    "    #\n",
    "    # Note: You should evaluate the training error on the first i training\n",
    "    #       examples (i.e., X[:i, :] and y[:i]).\n",
    "    #\n",
    "    #       For the cross-validation error, you should instead evaluate on\n",
    "    #       the _entire_ cross validation set (Xval and yval).\n",
    "    #\n",
    "    # Note: If you are using your cost function (linearRegCostFunction)\n",
    "    #       to compute the training and cross validation error, you should \n",
    "    #       call the function with the lambda argument set to 0. \n",
    "    #       Do note that you will still need to use lambda when running\n",
    "    #       the training to obtain the theta parameters.\n",
    "    #\n",
    "    # Hint: You can loop over the examples with the following:\n",
    "    #\n",
    "    #       for i = range(1,m):\n",
    "    #           # Compute train/cross validation errors using training examples \n",
    "    #           # X[:i, :] and y[:i], storing the result in \n",
    "    #           # error_train[i-1] and error_val[i-1]\n",
    "    #           ....\n",
    "    #           \n",
    "    #       end\n",
    "    #\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    # =========================================================================\n",
    "    \n",
    "    return error_train, error_val\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error_train, error_val = learning_curve(np.hstack([np.ones((m, 1)), X]), y, \n",
    "                                        np.hstack([np.ones((Xval.shape[0], 1)), Xval]), yval, \n",
    "                                        0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('\\n'.join(str(x) for x in zip(error_train, error_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(m-1), error_train, range(m-1), error_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Feature Mapping for Polynomial Regression \n",
    "  One solution to this is to use polynomial regression. You should now\n",
    "  complete poly_features to map each example into its powers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def poly_features(X_orig, p):\n",
    "    #POLYFEATURES Maps X (1D vector) into the p-th power\n",
    "    #   [X_poly] = POLYFEATURES(X, p) takes a vector X (size m) and\n",
    "    #   maps each example into its polynomial features where\n",
    "    #   X_poly[i, :] = [X[i] X[i]**2 X[i]**3 ...  X[i]*p]\n",
    "    #\n",
    "    \n",
    "    # You need to return the following variables correctly.\n",
    "    X_poly = np.zeros((len(X_orig), p))\n",
    "    \n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Given a vector X, return a matrix X_poly where the p-th \n",
    "    #               column of X contains the values of X to the p-th power.\n",
    "    #\n",
    "    # \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # =============================================================\n",
    "    \n",
    "    \n",
    "    return X_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map X onto Polynomial Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = 8\n",
    "X_poly = poly_features(X, p)\n",
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_normalize(X):\n",
    "    mu = np.mean(X, axis=0)\n",
    "    X_norm = X - mu\n",
    "    sigma = np.std(X_norm, axis=0)\n",
    "    sigma[sigma == 0] = 1\n",
    "    X_norm = X_norm / sigma\n",
    "    \n",
    "    return X_norm, mu, sigma    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_poly, mu, sigma = feature_normalize(X_poly)\n",
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add ones column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_poly = np.hstack([np.ones((m, 1)), X_poly])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map and normalize X_test and X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_poly_test = (poly_features(Xtest, p) - mu) / sigma\n",
    "X_poly_test = np.hstack([np.ones((X_poly_test.shape[0], 1)), X_poly_test])\n",
    "X_poly_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_poly_val = (poly_features(Xval, p) - mu) / sigma\n",
    "X_poly_val = np.hstack([np.ones((X_poly_val.shape[0], 1)), X_poly_val])\n",
    "X_poly_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Learning Curve for Polynomial Regression \n",
    "  Now, you will get to experiment with polynomial regression with multiple\n",
    "  values of lambda. The code below runs polynomial regression with \n",
    "  lambda = 0. You should try running the code with different values of\n",
    " lambda to see how the fit and learning curve change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_fit(ax, min_x, max_x, mu, sigma, theta, p):\n",
    "    x = np.linspace(min_x - 15, max_x + 15)\n",
    "    X_poly = (poly_features(x, p) - mu) / sigma\n",
    "    X_poly = np.c_[np.ones(len(x)), X_poly]\n",
    "    ax.plot(x, X_poly.dot(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training data and fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambda_ = 0\n",
    "res = trainLinearRegression(X_poly, y, lambda_)\n",
    "print(res)\n",
    "theta = res.x\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(X, y, marker='x', c='r')\n",
    "ax.set_xlabel('Change in water level')\n",
    "ax.set_ylabel('Water flowing out of the dam')\n",
    "plot_fit(ax, np.min(X), np.max(X), mu, sigma, theta, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial Regression Learning Curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "error_train, error_val = learning_curve(X_poly, y, X_poly_val, yval, lambda_)\n",
    "plt.plot(range(m-1), error_train)\n",
    "plt.plot(range(m-1), error_val)\n",
    "plt.legend(['Train', 'Cross Validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 8: Validation for Selecting Lambda \n",
    "  You will now implement validation_curve to test various values of \n",
    "  lambda on a validation set. You will then use this to select the\n",
    "  \"best\" lambda value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation_curve(X, y, Xval, yval):\n",
    "    #VALIDATIONCURVE Generate the train and validation errors needed to\n",
    "    #plot a validation curve that we can use to select lambda\n",
    "    #   [lambda_vec, error_train, error_val] = ...\n",
    "    #       VALIDATIONCURVE(X, y, Xval, yval) returns the train\n",
    "    #       and validation errors (in error_train, error_val)\n",
    "    #       for different values of lambda. You are given the training set (X,\n",
    "    #       y) and validation set (Xval, yval).\n",
    "    #\n",
    "\n",
    "    # Selected values of lambda (you should not change this)\n",
    "    lambda_vec = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
    "    \n",
    "    # You need to return these variables correctly.\n",
    "    error_train = np.zeros(len(lambda_vec))\n",
    "    error_val = np.zeros(len(lambda_vec))\n",
    "    \n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Fill in this function to return training errors in \n",
    "    #               error_train and the validation errors in error_val. The \n",
    "    #               vector lambda_vec contains the different lambda parameters \n",
    "    #               to use for each calculation of the errors, i.e, \n",
    "    #               error_train(i), and error_val(i) should give \n",
    "    #               you the errors obtained after training with \n",
    "    #               lambda = lambda_vec(i)\n",
    "    #\n",
    "    # Note: You can loop over lambda_vec with the following:\n",
    "    #\n",
    "    #       for i in range(len(lambda_vec)):\n",
    "    #           lambda = lambda_vec[i];\n",
    "    #           # Compute train / val errors when training linear \n",
    "    #           # regression with regularization parameter lambda\n",
    "    #           # You should store the result in error_train[i]\n",
    "    #           # and error_val[i]\n",
    "    #           ....\n",
    "    #           \n",
    "    #       end\n",
    "    #\n",
    "    #\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # =========================================================================\n",
    "    \n",
    "    return lambda_vec, error_train, error_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lambda_vec, error_train, error_val = validation_curve(X_poly, y, X_poly_val, yval)\n",
    "plt.plot(lambda_vec, error_train, lambda_vec, error_val)\n",
    "plt.ylim([0, 20])\n",
    "plt.legend(['Training error', 'Cross validation error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lambda_ = 3\n",
    "res = trainLinearRegression(X_poly, y, lambda_)\n",
    "theta = res.x\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.scatter(X, y, marker='x', c='r')\n",
    "ax.set_xlabel('Change in water level')\n",
    "ax.set_ylabel('Water flowing out of the dam')\n",
    "plot_fit(ax, np.min(X), np.max(X), mu, sigma, theta, p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
